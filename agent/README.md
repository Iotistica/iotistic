# Iotistic Agent

Edge device agent for the Iotistic IoT platform. Provides container orchestration, cloud synchronization, device provisioning, and real-time monitoring for IoT devices running on Raspberry Pi, x86_64, and other edge hardware.

## 🎯 Quick Start

### CLI Tool - iotctl

The agent includes a powerful CLI tool for device management:

```bash
# Inside the Docker container

# Provisioning commands
iotctl provision <key>            # Provision with cloud (--api, --name, --type options)
iotctl provision status           # Check provisioning state
iotctl deprovision                # Remove cloud registration (keeps UUID/deviceApiKey)
iotctl factory-reset              # WARNING: Complete wipe! Deletes everything

# Configuration commands
iotctl config show                # Show all configuration
iotctl config set-api <url>       # Update cloud API endpoint
iotctl config get-api             # Show current API endpoint
iotctl config set <key> <value>   # Set any config value
iotctl config get <key>           # Get specific config value
iotctl config reset               # Reset to defaults

# Device management
iotctl status                     # Device health and status
iotctl diagnostics                # Run full system diagnostics (API, DB, MQTT, cloud)
iotctl diag                       # Short alias for diagnostics
iotctl restart                    # Restart the agent
iotctl logs --follow              # View agent logs (use from host: docker logs -f agent-1)
iotctl logs -n 50                 # Show last 50 log lines

# Application-level commands (manage entire stacks)
iotctl apps list                  # List all apps and services
iotctl apps start 1001            # Start all services in app
iotctl apps stop 1001             # Stop all services in app
iotctl apps restart 1001          # Restart entire app stack
iotctl apps info 1001             # Show app details
iotctl apps purge 1001            # Remove app + volumes

# Service-level commands (manage individual containers)
iotctl services list              # List all services/containers
iotctl services list 1001         # Services in specific app
iotctl services start web-1       # Start one container
iotctl services stop api-2        # Stop one container
iotctl services restart db-1      # Restart one container
iotctl services logs web-1 -f     # Follow container logs
iotctl services info web-1        # Detailed service info

# System
iotctl help                       # Show all commands
iotctl version                    # Show CLI version
```

**Key Features:**
- ✅ REST client to Device API (port 48484 by default)
- ✅ Structured logging (no emojis, JSON context)
- ✅ Provisioning with two-phase authentication
- ✅ Factory reset support
- ✅ No config files - all data from Device API/database
- ✅ **Dual-level control**: Apps (stacks) + Services (containers)

**Architecture:**
- **App** = Collection of one or more services (like docker-compose stack)
- **Service** = Individual Docker container
- **Apps commands** = Manage entire stacks (all containers in app)
- **Services commands** = Manage individual containers

**Example Workflow:**
```bash
# Check device status
docker exec agent-1 iotctl status
# [INFO] Agent running {"uuid":"1dc6ce29-be81-49ee-aad7-b2d317a96fbb"}
# [INFO] Applications {"configured":0,"runningServices":0}

# List all apps and their services
docker exec agent-1 iotctl apps list

# List individual services
docker exec agent-1 iotctl services list

# Start entire app stack
docker exec agent-1 iotctl apps start 1001

# Restart just one service in the stack
docker exec agent-1 iotctl services restart myapp-web-1

# Follow logs from specific service
docker exec agent-1 iotctl services logs myapp-api-2 -f
```

### Anomaly Detection (Edge AI)

Real-time anomaly detection monitors device metrics using multiple ML algorithms running **locally on the device** (no cloud dependency):

**Monitored Metrics:**
- **System**: CPU usage, CPU temperature, memory usage, storage usage, network metrics
- **Sensors**: Custom sensor data via MQTT (temperature, humidity, pressure, gas, etc.)
- **Quality Tracking**: Data quality indicators (GOOD/UNCERTAIN/BAD)

**Detection Algorithms:**
1. **Z-Score** - Statistical deviation from baseline (σ > 3)
2. **MAD (Median Absolute Deviation)** - Robust outlier detection
3. **IQR (Interquartile Range)** - Quartile-based outliers (1.5× IQR rule)
4. **Rate of Change** - Sudden spikes/drops detection
5. **ML Predictions** - LSTM-based time-series forecasting (optional)

**Configuration:**
```bash
# Environment variables
ANOMALY_DETECTION_ENABLED=true           # Enable/disable anomaly detection
ANOMALY_WINDOW_SIZE=100                  # Samples for baseline calculation
ANOMALY_SENSITIVITY=medium               # low|medium|high (affects thresholds)
ANOMALY_ML_ENABLED=false                 # Enable LSTM predictions (experimental)
```

**Automatic Cloud Reporting:**
```typescript
// Agent automatically reports anomalies to cloud every 60s
CloudSync.getSummaryForReport(10);  // Last 10 anomalies included in report
```

**Example Anomaly Output:**
```json
{
  "timestamp": 1736985600000,
  "source": "system",
  "metric": "memory_percent",
  "value": 95.2,
  "method": "zscore",
  "severity": "critical",
  "confidence": 0.95,
  "deviation": 16.97,
  "quality": "GOOD",
  "threshold": 3.0
}
```

**Severity Levels:**
- `info` - Minor deviation (notification only)
- `warning` - Moderate deviation (attention needed)
- `critical` - Major deviation (immediate action required)

**Integration with Simulation:**
- Anomaly detection works seamlessly with simulation mode
- Test detection algorithms by injecting controlled anomalies
- Verify cloud reporting without real hardware

**Wiring:**
- System metrics automatically fed to anomaly detector
- Sensor data can be fed via `AnomalyDetectionService.processDataPoint()`
- Results stored in memory (last 1000 anomalies) + sent to cloud

### Simulation Mode

Unified testing framework for realistic sensor data and anomaly injection:

**Configuration:**
```bash
# docker-compose.yml
SIMULATION_MODE=true
SIMULATION_CONFIG='{"scenarios":{"anomaly_injection":{"enabled":true,"metrics":["cpu_temp","memory_percent"],"pattern":"spike","intervalMs":30000,"magnitude":3},"sensor_data":{"enabled":true,"pattern":"realistic","publishIntervalMs":10000}}}'
```

**Features:**
- 📊 **Realistic sensor data** - BME688-style temperature, humidity, pressure, gas readings
- 🔥 **Anomaly injection** - Configurable spikes, drops, or drift patterns
- 🎭 **Multiple patterns** - Random, sine wave, realistic variations
- ⏱️ **Configurable intervals** - Control data generation frequency
- 🎯 **Metric targeting** - Inject anomalies into specific metrics

**Patterns:**
- `spike` - Sudden short-lived increases
- `drop` - Sudden short-lived decreases  
- `drift` - Gradual trending changes
- `random` - Chaotic variations
- `sine` - Cyclical patterns

**Use Cases:**
- Testing anomaly detection algorithms
- Stress testing cloud sync
- UI/dashboard development without hardware
- CI/CD integration testing

## ⚡ Performance Optimizations

### System Metrics Collection

**Graceful Degradation** - Never crash, always return data:

Every metric collection is wrapped in a safe executor that prevents exceptions from propagating. If any single metric fails (sensor offline, permission denied, etc.), the system returns a sensible fallback value and continues collection:

```typescript
const safe = async <T>(fn: () => Promise<T>, fallback: T) => {
  try { return await fn(); }
  catch { return fallback; }
};

// Example: Memory collection fails? Return zeros instead of crashing
const memory = await safe(getMemoryInfo, { used: 0, total: 0, percent: 0 });
```

**Why This Matters:**
- **Edge Reliability**: Sensors fail, hardware glitches - partial data is better than no data
- **Never Crash**: Metrics collection always succeeds, even with failing hardware
- **Production Ready**: Cloud gets the data it can, logs the rest
- **Debugging Friendly**: Fallback values (0, null, [], 'unknown') are easily identifiable

**Fallback Values:**
- `cpuUsage`: 0
- `cpuTemp`: null (may not exist on all platforms)
- `cpuCores`: 1
- `memoryInfo`: { used: 0, total: 0, percent: 0 }
- `storageInfo`: { used: null, total: null, percent: null }
- `uptime`: 0
- `hostname`: 'unknown'
- `undervolted`: false
- `networkInterfaces`: []
- `topProcesses`: []

**Static Value Caching** - Immutable or rarely-changing system properties are cached to dramatically improve performance on subsequent calls:

**Truly Static Values** (cached indefinitely after first retrieval):
- `hostname` - Device hostname (never changes)
- `cpuCores` - Number of CPU cores (never changes)

**Semi-Static Values** (cached with auto-expiry):
- `networkInterfaces` - Network interface configuration (30-second TTL)
  - Auto-expires to detect WiFi SSID changes, VPN connections, docker0 appearance, etc.

**Dynamic Values** (fetched every collection):
- `cpuUsage`, `cpuTemp` - Current CPU state
- `memoryInfo` - Current RAM usage
- `storageInfo` - Current disk usage
- `uptime` - System uptime

**Performance Results:**
- **First collection**: ~2100ms (caches static values)
- **Subsequent collections**: ~400-800ms (13x faster!)
- **Overall improvement**: 92% reduction from original 5000ms baseline

**Platform-Specific Optimizations:**

**Process Collection** - Expensive `topProcesses` data collection is platform-aware:
```bash
# Default behavior:
# - Windows (development): Processes DISABLED (slow on Windows)
# - Linux (production): Processes ENABLED (fast on Linux)

# Override via environment variable:
COLLECT_TOP_PROCESSES=true   # Force enable (e.g., Windows debugging)
COLLECT_TOP_PROCESSES=false  # Force disable (e.g., Linux resource constraints)
```

**Timing Instrumentation:**
- All metric collection operations timed automatically
- Operations >500ms logged with detailed breakdown
- Top 10 slowest operations shown for visibility
- Helps identify platform-specific performance issues

**Garbage Collection Optimization:**
- **Zero-allocation hot paths** where possible
- Single-pass filter+score+sort in `getTopProcesses` (no intermediate arrays)
- Pre-allocated result arrays to avoid dynamic resizing
- In-place sorting to reduce memory pressure

**Platform CPU Normalization:**
- **Critical Fix**: systeminformation reports CPU differently across platforms
  - **Linux/Unix**: CPU % is per-core (e.g., 400% on 4-core system means 100% per core)
  - **Windows**: CPU % is per-system (e.g., 100% max total)
- **Solution**: Normalize Linux CPU by dividing by core count
- **Result**: Fair process comparison across all platforms
- **Example**: 100% CPU on one core = 25% on 4-core system (comparable to Windows)

**Before (incorrect comparison)**:
```typescript
// Linux process using 400% (1 core fully loaded on 4-core)
// Windows process using 25% (same workload)
// These look vastly different but are equivalent!
```

**After (normalized)**:
```typescript
const normalizedCpu = proc.cpu / cpuCoreCount;  // Linux only
// Both platforms now report 25% for same workload
// Fair scoring and ranking across platforms
```

**Before (multiple allocations)**:
```typescript
const filtered = processes.filter(...)  // New array #1
const sorted = filtered.sort(...)       // Mutates but creates comparisons
const top10 = sorted.slice(0, 10)       // New array #2
const formatted = top10.map(...)        // New array #3
```

**After (minimal allocations)**:
```typescript
const scored = [];                      // Single working array
for (const proc of processes) {
  // Inline filter + score calculation
}
scored.sort(...)                        // In-place sort
// Direct format into pre-allocated result
```

**Why This Matters:**
- Reduced GC pressure on resource-constrained devices
- Lower CPU usage during metrics collection
- More predictable latency (fewer GC pauses)
- Critical for edge devices with limited RAM
- Accurate process ranking regardless of platform

**Why This Matters:**
- Windows development: Sub-second metrics without sacrificing functionality
- Linux production: Full process data + fast performance
- Raspberry Pi: Optimized for resource-constrained edge devices

### OS-Specific Extended Metrics

**Enhanced platform-specific metrics** provide deeper insights into system behavior beyond standard monitoring:

**Linux Extended Metrics:**

1. **Load Average** (`load_average: number[]`)
   - 1, 5, and 15-minute load averages from `os.loadavg()`
   - Measures system resource demand over time
   - Example: `[0.5, 0.8, 1.2]` indicates increasing load

2. **Disk I/O** (`disk_io: { read: number; write: number }`)
   - Real-time disk operations per second
   - Uses `systeminformation.disksIO()`
   - Helps identify I/O bottlenecks

3. **CPU Throttling** (`cpu_throttling: { current_freq: number; max_freq: number }`)
   - Current vs. maximum CPU frequency (MHz)
   - Reads from `/sys/devices/system/cpu/cpu0/cpufreq/`
   - Detects thermal throttling or power-saving modes
   - Example: `{ current_freq: 1800, max_freq: 2400 }` = 75% max speed

**Windows Extended Metrics:**

1. **GPU Temperature** (`gpu_temp: number`)
   - Graphics card temperature in Celsius
   - Queries `MSAcpi_ThermalZoneTemperature` via WMI
   - Requires elevated privileges on some systems
   - Returns `undefined` if unavailable

2. **Disk Metrics** (`disk_metrics: { read_ops: number; write_ops: number }`)
   - Disk read/write operations per second
   - Uses `systeminformation.disksIO()`
   - Monitors storage subsystem activity

**Usage Example:**
```typescript
const metrics = await getSystemMetrics();

if (metrics.extended) {
  // Linux-specific
  if (metrics.extended.load_average) {
    const [load1, load5, load15] = metrics.extended.load_average;
    console.log(`Load: ${load1.toFixed(2)} (1m), ${load5.toFixed(2)} (5m)`);
  }
  
  if (metrics.extended.cpu_throttling) {
    const { current_freq, max_freq } = metrics.extended.cpu_throttling;
    const throttlePercent = ((max_freq - current_freq) / max_freq * 100).toFixed(1);
    console.log(`CPU throttled by ${throttlePercent}%`);
  }
  
  // Windows-specific
  if (metrics.extended.gpu_temp !== undefined) {
    console.log(`GPU Temperature: ${metrics.extended.gpu_temp}°C`);
  }
}
```

**Why This Matters:**
- **Performance Diagnosis**: Load average and disk I/O reveal system bottlenecks
- **Thermal Management**: CPU throttling and GPU temps detect overheating
- **Platform Optimization**: Leverage OS-specific APIs for richer telemetry
- **Graceful Degradation**: All extended metrics optional - returns `undefined` if unavailable
- **Production Insights**: Go beyond basic metrics without sacrificing reliability

## 🔐 MQTTS/TLS Setup

Secure MQTT communication with TLS encryption using self-signed or commercial certificates.

### Overview

The agent supports MQTTS (MQTT over TLS) with automatic certificate handling during provisioning. When a device is provisioned, it receives:
- Broker URL (`mqtts://mosquitto:8883`)
- MQTT credentials (username/password)
- **Broker configuration** including CA certificate for TLS verification

### Architecture Flow

```
1. API Database (PostgreSQL)
   └── system_config table
       └── mqtt.brokers.1 (JSONB)
           └── Contains: host, port, protocol, caCert, useTls, verifyCertificate

2. Provisioning (API → Agent)
   └── POST /api/provisioning/v2/register
       └── Response includes mqtt.brokerConfig with CA certificate

3. Agent Storage (SQLite)
   └── device table
       └── mqttBrokerConfig column (JSON string)
           └── Stores: protocol, host, port, useTls, caCert, verifyCertificate

4. Agent MQTT Connection
   └── agent.ts: initializeMqttManager()
       └── Reads mqttBrokerConfig from database
       └── Applies TLS options: { ca: caCert, rejectUnauthorized: verifyCertificate }
```

### Setup Steps

#### 1. Generate CA Certificate (Self-Signed)

```bash
# Create certs directory
mkdir -p certs

# Generate CA private key and certificate
openssl req -new -x509 -days 365 -extensions v3_ca \
  -keyout certs/ca.key -out certs/ca.crt \
  -subj "/CN=Iotistic CA"

# Generate server certificate signed by CA
openssl genrsa -out certs/server.key 2048
openssl req -new -out certs/server.csr -key certs/server.key \
  -subj "/CN=mosquitto"
openssl x509 -req -in certs/server.csr -CA certs/ca.crt \
  -CAkey certs/ca.key -CAcreateserial -out certs/server.crt \
  -days 365
```

#### 2. Configure Mosquitto Broker

```conf
# mosquitto/mosquitto.conf

# Standard MQTT (port 1883)
listener 1883
protocol mqtt
allow_anonymous false

# MQTTS with TLS (port 8883)
listener 8883
protocol mqtt
cafile /mosquitto/certs/ca.crt
certfile /mosquitto/certs/server.crt
keyfile /mosquitto/certs/server.key
require_certificate false
use_identity_as_username false
allow_anonymous false

# Authentication backend (PostgreSQL)
auth_plugin /mosquitto/go-auth.so
auth_opt_backends postgres
auth_opt_pg_host postgres
auth_opt_pg_port 5432
auth_opt_pg_dbname iotistic
auth_opt_pg_user postgres
auth_opt_pg_password password
auth_opt_pg_userquery SELECT password_hash FROM mqtt_users WHERE username = $1 AND enabled = TRUE LIMIT 1
auth_opt_pg_aclquery SELECT 1 FROM mqtt_acls WHERE username = $1 AND topic = $2 AND rw >= $3 LIMIT 1
```

#### 3. Store CA Certificate in API Database

```sql
-- Run migration 057 or manually insert
UPDATE system_config
SET value = jsonb_set(
  value,
  '{caCert}',
  to_jsonb('-----BEGIN CERTIFICATE-----
...your CA certificate content...
-----END CERTIFICATE-----'::text)
)
WHERE key = 'mqtt.brokers.1';
```

Or use the migration:
```bash
cd api
npx knex migrate:latest  # Runs 057_add_mqtt_ca_certificate.sql
```

#### 4. Verify API Sends CA Certificate

Check `api/src/utils/mqtt-broker-config.ts`:
```typescript
export function formatBrokerConfigForClient(config: any) {
  return {
    protocol: config.protocol,
    host: config.host,
    port: config.port,
    useTls: config.useTls ?? config.use_tls,
    verifyCertificate: config.verifyCertificate ?? config.verify_certificate,
    // CA certificate included for TLS
    ...(config.caCert && { caCert: config.caCert }),
    // Other fields...
  };
}
```

#### 5. Agent Applies TLS Options

Check `agent/src/agent.ts`:
```typescript
// Build MQTT connection options
const mqttOptions: any = {
  clientId: `device_${this.deviceInfo.uuid}`,
  username: mqttUsername,
  password: mqttPassword,
};

// Add TLS options if broker config specifies TLS
if (this.deviceInfo.mqttBrokerConfig?.useTls && 
    this.deviceInfo.mqttBrokerConfig.caCert) {
  mqttOptions.ca = this.deviceInfo.mqttBrokerConfig.caCert; // String (PEM format)
  mqttOptions.rejectUnauthorized = this.deviceInfo.mqttBrokerConfig.verifyCertificate;
}

await mqttManager.connect(mqttBrokerUrl, mqttOptions);
```

### Provisioning with MQTTS

#### New Device Provisioning

```bash
# Start agent with provisioning key
docker run -e PROVISIONING_API_KEY=your-key \
  -e CLOUD_API_ENDPOINT=http://api:3002 \
  iotistic/agent:latest

# Agent auto-provisions and receives:
# - mqtt.brokerConfig.protocol = "mqtts"
# - mqtt.brokerConfig.host = "mosquitto"
# - mqtt.brokerConfig.port = 8883
# - mqtt.brokerConfig.caCert = "-----BEGIN CERTIFICATE-----\n..."
# - mqtt.brokerConfig.useTls = true
# - mqtt.brokerConfig.verifyCertificate = true
```

#### Re-Provisioning Existing Devices

If you added MQTTS **after** devices were already provisioned:

```bash
# Option 1: Delete device database (forces re-provisioning)
docker exec agent-1 rm /app/data/device.sqlite
docker restart agent-1

# Option 2: Manual database update (advanced)
docker exec agent-1 sqlite3 /app/data/device.sqlite
# UPDATE device SET mqttBrokerConfig = '{"protocol":"mqtts",...}';
```

### Verification

#### Check Agent Logs

```bash
docker logs agent-1 | grep -i "mqtt\|tls"

# Expected output:
# MQTT TLS enabled {"protocol":"mqtts","verifyCertificate":true,"hasCaCert":true}
# MQTT Manager connected {"brokerUrl":"mqtts://mosquitto:8883"}
```

#### Check Agent Database

```bash
# Create check script
cat > check-mqtt-config.js << 'EOF'
const Database = require('better-sqlite3');
const db = new Database('/app/data/device.sqlite');
const row = db.prepare('SELECT mqttBrokerConfig FROM device LIMIT 1').get();
console.log(JSON.stringify(JSON.parse(row.mqttBrokerConfig), null, 2));
EOF

# Run inside container
docker exec agent-1 node /app/check-mqtt-config.js

# Expected output:
# {
#   "protocol": "mqtts",
#   "host": "mosquitto",
#   "port": 8883,
#   "useTls": true,
#   "caCert": "-----BEGIN CERTIFICATE-----\n...",
#   "verifyCertificate": true
# }
```

#### Test MQTTS Connection

```bash
# Install mosquitto clients
apt-get install mosquitto-clients

# Test with CA certificate
mosquitto_pub -h localhost -p 8883 \
  --cafile ./certs/ca.crt \
  -u device_uuid -P mqtt_password \
  -t test -m "Hello MQTTS"

# Should succeed with no certificate errors
```

### Troubleshooting

#### Error: "self-signed certificate in certificate chain"

**Cause**: Agent doesn't have CA certificate or not applying TLS options.

**Fix**:
1. Check API database has CA cert: `SELECT value->'caCert' FROM system_config WHERE key = 'mqtt.brokers.1'`
2. Check agent database: Run check script above
3. Verify agent code applies TLS: Look for "MQTT TLS enabled" in logs
4. Re-provision device if needed

#### Error: "unable to verify the first certificate"

**Cause**: CA certificate doesn't match server certificate.

**Fix**:
1. Regenerate server cert signed by same CA
2. Ensure Mosquitto using correct `cafile`, `certfile`, `keyfile`
3. Restart Mosquitto: `docker restart iotistic-mosquitto`

#### Error: "ECONNREFUSED" on port 8883

**Cause**: Mosquitto not listening on MQTTS port.

**Fix**:
1. Check Mosquitto config has `listener 8883` section
2. Verify port exposed: `docker port iotistic-mosquitto 8883`
3. Check Mosquitto logs: `docker logs iotistic-mosquitto`

#### Missing CA Certificate in Provisioning Response

**Cause**: `formatBrokerConfigForClient()` not handling camelCase fields.

**Fix**: Update `api/src/utils/mqtt-broker-config.ts`:
```typescript
export function formatBrokerConfigForClient(config: any) {
  const caCert = config.caCert ?? config.ca_cert ?? null;
  return {
    // ... other fields
    ...(caCert && { caCert })
  };
}
```

### Production Considerations

**Certificate Rotation**:
- Update CA cert in `system_config` table
- Re-provision devices OR push new config via cloud sync
- Mosquitto auto-reloads on SIGHUP

**Client Certificates** (mutual TLS):
- Set `require_certificate true` in Mosquitto config
- Generate client certs for each device
- Include `clientCert` and `clientKey` in broker config

**Commercial Certificates**:
- Use Let's Encrypt or commercial CA instead of self-signed
- Update `caCert` in database to trusted root CA
- Clients automatically trust well-known CAs

**Certificate Expiration**:
- Monitor expiration dates
- Automate renewal (certbot for Let's Encrypt)
- Plan device update rollout before expiry

## 🔄 Cloud Sync Reliability

Robust cloud communication with automatic failure recovery and protection mechanisms.

### Circuit Breaker Protection

The agent implements circuit breakers for both poll and report operations to prevent hammering the cloud API during outages:

**Configuration:**
- **Failure Threshold**: 10 consecutive failures
- **Cooldown Period**: 5 minutes
- **Auto-Reset**: Circuit closes on first success after cooldown

**How It Works:**
```typescript
// Poll loop protection
if (pollCircuit.isOpen()) {
  // Skip operation, cooling down
  logger.warn('Poll circuit breaker open', {
    cooldownRemainingSec: circuit.getCooldownRemaining() / 1000,
    failureCount: circuit.getFailureCount()
  });
  return;
}

try {
  await pollTargetState();
  pollCircuit.recordSuccess(); // Reset counter
} catch (error) {
  const opened = pollCircuit.recordFailure();
  if (opened) {
    logger.error('Circuit breaker tripped - stopping polls for 5min');
  }
}
```

**Benefits:**
- Stops wasting resources on unreachable API
- Prevents log spam during prolonged outages
- Automatic recovery when API returns
- Independent circuits for poll/report (isolation)

### Request Deduplication

Async locks prevent overlapping requests if operations take longer than the poll/report interval:

**Problem Scenario:**
```
00:00 - Poll starts (takes 90s due to slow network)
01:00 - Timer fires, another poll attempts to start
01:30 - First poll still running, second poll blocked
```

**Solution:**
```typescript
// Before executing
if (pollLock.isLocked()) {
  logger.warn('Poll already in progress, skipping');
  return; // Prevents overlap
}

// Execute with lock
await pollLock.tryExecute(async () => {
  await pollTargetState(); // Guaranteed single execution
});
```

**Benefits:**
- No duplicate API calls
- No race conditions
- Predictable resource usage
- Clean error messages

### Error Counter Capping

Error counters are capped at 10 attempts to prevent overflow and extremely long backoff delays:

```typescript
// Before
this.pollErrors++; // Could increment forever

// After
this.pollErrors = Math.min(this.pollErrors + 1, 10); // Capped at 10
```

**Backoff Schedule** (with 15s base, 2x multiplier, 15min max):
| Attempt | Delay (without jitter) |
|---------|------------------------|
| 1       | 15s                    |
| 2       | 30s                    |
| 3       | 60s (1min)             |
| 4       | 120s (2min)            |
| 5       | 240s (4min)            |
| 6       | 480s (8min)            |
| 7+      | 900s (15min) - capped  |

**Benefits:**
- Prevents integer overflow
- Reasonable max retry delay (15min)
- Predictable behavior
- Faster recovery after long outages

### Combined Protection

All three mechanisms work together for robust failure handling:

```typescript
// Example: API down for 30 minutes

// 00:00 - Poll fails (attempt 1, backoff 15s)
// 00:15 - Poll fails (attempt 2, backoff 30s)
// 00:45 - Poll fails (attempt 3, backoff 1min)
// ... continues with exponential backoff ...
// 10:00 - Poll fails (attempt 10, circuit opens)
// 10:00 - Circuit breaker activated (5min cooldown)
// 15:00 - Circuit cooldown expires, resume polling
// 15:00 - Poll succeeds, all counters reset

// During this time:
// - No duplicate requests (async lock)
// - Backoff capped at 15min (error cap)
// - Stopped trying after 10 failures (circuit breaker)
```

### Monitoring

All protection mechanisms emit detailed logs for visibility:

```json
{
  "component": "cloudSync",
  "operation": "poll-circuit-open",
  "cooldownRemainingSec": 180,
  "failureCount": 10
}

{
  "component": "cloudSync", 
  "operation": "poll-circuit-trip",
  "consecutiveFailures": 10,
  "cooldownMs": 300000,
  "cooldownMin": 5
}

{
  "component": "cloudSync",
  "operation": "poll-skip-locked"
}
```

### Configuration

All protection is automatic with sensible defaults:

```typescript
// Circuit breaker (in constructor)
this.pollCircuit = new CircuitBreaker(
  10,              // maxFailures
  5 * 60 * 1000    // cooldownMs (5 minutes)
);

// Error counter cap (in loop)
this.pollErrors = Math.min(this.pollErrors + 1, 10);

// Async lock (automatic)
this.pollLock = new AsyncLock();
```

No environment variables needed - works out of the box!

### Utilities

All protection utilities are reusable via `retry-policy.ts`:

```typescript
import { CircuitBreaker, AsyncLock, isAuthError } from '../utils/retry-policy';

// Circuit breaker
const circuit = new CircuitBreaker(10, 5 * 60 * 1000);
if (circuit.isOpen()) { /* skip */ }

// Async lock
const lock = new AsyncLock();
await lock.tryExecute(async () => { /* protected operation */ });

// Error classification
if (isAuthError(error)) { /* refresh credentials */ }
```

---

## 🐳 Docker Integration

Deploy, update, and manage containers with full Docker and Kubernetes support.

### Quick Start (30 seconds)

```bash
# Make sure Docker is running
docker ps

# Deploy your first container!
npx tsx quick-start.ts

# Visit http://localhost:8080
```

### Features

✅ **Docker Integration** - Uses dockerode for actual Docker operations  
✅ **State Reconciliation** - Automatically calculates and applies changes  
✅ **Multi-Container Apps** - Deploy complex stacks (like docker-compose)  
✅ **Rolling Updates** - Zero-downtime container updates  
✅ **REST API** - Control via HTTP (see `api/` folder)  
✅ **Simulated Mode** - Test without Docker  

---


## Project Structure

```
standalone-application-manager/
├── src/
│   ├── application-manager.ts  # Main application manager logic
│   ├── app.ts                  # App class for managing application state
│   ├── composition-steps.ts    # Composition step generation and execution
│   ├── types.ts                # TypeScript type definitions
│   ├── stubs.ts                # Stub implementations for dependencies
│   └── index.ts                # Main entry point
├── examples/
│   └── basic-usage.ts          # Example usage
├── package.json
├── tsconfig.json
└── README.md
```

## Installation

```bash
cd standalone-application-manager
npm install
```

## Building

```bash
npm run build
```

This will compile the TypeScript files to JavaScript in the `dist/` directory.

## Testing

Run the test suite to verify the application manager works correctly:

```bash
# Install test runner
npm install -D tsx

# Run simple test
npx tsx test/simple-test.ts

# Run test with mock data
npx tsx test/mock-data-test.ts

# Run comprehensive test
npx tsx test/basic-test.ts
```

See [test/README.md](test/README.md) for more testing options.

## Usage

### Basic Example

```typescript
import applicationManager from 'standalone-application-manager';

// Initialize the application manager
await applicationManager.initialized();

// Get current applications
const currentApps = await applicationManager.getCurrentApps();

// Get target applications (from your configuration source)
const targetApps = await applicationManager.getTargetApps();

// Calculate required steps to reach target state
const steps = await applicationManager.getRequiredSteps(
	currentApps,
	targetApps,
	false, // keepImages
	false, // keepVolumes
	false  // force
);

// Execute each step
for (const step of steps) {
	await applicationManager.executeStep(step);
}
```

### Listening to Events

```typescript
// Listen for application state changes
applicationManager.on('change', (report) => {
	console.log('Application state changed:', report);
});
```

## Key Concepts

### Applications

Applications are composed of:
- **Services**: Docker containers running your application code
- **Networks**: Network configurations for inter-service communication
- **Volumes**: Persistent storage for application data

### Composition Steps

The application manager generates "composition steps" that represent atomic operations needed to transition from the current state to the target state. Step types include:

- `fetch`: Download a container image
- `start`: Start a service
- `stop`: Stop a service
- `kill`: Kill a service (forceful stop)
- `remove`: Remove a stopped service
- `createNetwork`: Create a network
- `createVolume`: Create a volume
- `removeNetwork`: Remove a network
- `removeVolume`: Remove a volume
- `updateMetadata`: Update service metadata
- `takeLock`: Acquire update locks
- `releaseLock`: Release update locks

### Update Strategies

The manager supports different update strategies:
- **Download then kill**: Download new image first, then replace
- **Kill then download**: Stop service first, then download
- **Delete then download**: Remove everything first (for major changes)
- **Handover**: Gradual transition between versions

## Architecture

### Simplified Design

This standalone version uses stub implementations for external dependencies like:
- Database operations (replaced with in-memory or no-op stubs)
- Docker API calls (stubbed for demonstration)
- System configuration (using defaults)
- Logging infrastructure (console-based)

### Extension Points

To use this in production, you would need to implement:

1. **Docker Integration**: Replace stubs in `stubs.ts` with real Docker API calls using `dockerode`
2. **Database**: Implement actual persistence for target state and configuration
3. **Network Layer**: Implement real network management
4. **Volume Management**: Implement real volume lifecycle management
5. **Image Management**: Implement real image download, delta updates, and cleanup
6. **Service Manager**: Implement actual container lifecycle management
7. **Logging**: Integrate with your logging infrastructure

## API Reference

### Main Functions

#### `initialized(): Promise<void>`
Initializes the application manager. Must be called before other operations.

#### `getCurrentApps(): Promise<InstancedAppState>`
Returns the current state of all applications.

#### `getTargetApps(): Promise<TargetApps>`
Returns the desired target state for applications.

#### `getRequiredSteps(currentApps, targetApps, keepImages?, keepVolumes?, force?): Promise<CompositionStep[]>`
Calculates the steps needed to transition from current to target state.

Parameters:
- `currentApps`: Current application state
- `targetApps`: Desired application state
- `keepImages`: Don't remove unused images (optional, default: false)
- `keepVolumes`: Don't remove unused volumes (optional, default: false)
- `force`: Force updates even if locked (optional, default: false)

#### `executeStep(step, options?): Promise<void>`
Executes a single composition step.

#### `setTarget(apps, source, transaction): Promise<void>`
Sets the target state for applications.

#### `getState(): Promise<AppState>`
Returns the current state formatted for reporting.

### Events

The application manager emits the following events:

- `change`: Emitted when application state changes

## Development

### Project Setup

1. Clone or extract to a separate folder
2. Install dependencies: `npm install`
3. Build: `npm run build`
4. Watch mode: `npm run watch`

### Testing

Currently, this is a demonstration extraction. To make it production-ready:

1. Implement actual Docker operations
2. Add comprehensive unit tests
3. Add integration tests with real Docker daemon
4. Implement error handling and retry logic
5. Add monitoring and observability

## Limitations

This standalone version is a simplified extraction that:
- Uses stub implementations for external dependencies
- Lacks full error handling
- Requires additional work to be production-ready


## License

Apache-2.0

## Contributing

Contributions welcome! Please open an issue or pull request on the [Iotistic repository](https://github.com/Iotistica/iotistic).

